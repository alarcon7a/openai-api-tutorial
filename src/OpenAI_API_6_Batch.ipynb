{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz5kIwJdldskcqrb0UUusg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alarcon7a/openai-api-tutorial/blob/main/src/OpenAI_API_6_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "-aROO1-Pv2j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y179E7vv05F"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI, AzureOpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "api_key_openai = userdata.get(\"OPENAI_API_KEY\")\n",
        "client_openai = OpenAI(api_key=api_key_openai)"
      ],
      "metadata": {
        "id": "Xs77rXWnwE_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creando nuestro archivo jsonl"
      ],
      "metadata": {
        "id": "6UCR4ZSPr8kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def create_batch_file(requests: List[Dict[str, str]], output_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Crea un archivo JSONL para el batch con las solicitudes especificadas.\n",
        "\n",
        "    Args:\n",
        "        requests: Lista de diccionarios con id y content\n",
        "        output_path: Ruta donde se guardar√° el archivo JSONL\n",
        "\n",
        "    Returns:\n",
        "        Ruta del archivo creado\n",
        "    \"\"\"\n",
        "    print('Creando archivo de batch...')\n",
        "\n",
        "    # Crear contenido JSONL\n",
        "    jsonl_lines = []\n",
        "    for req in requests:\n",
        "        batch_request = {\n",
        "            \"custom_id\": req[\"id\"],\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": \"gpt-4o\",  # Aseg√∫rate de usar un modelo compatible con Batch API\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": \"Responde como un pirata\"},\n",
        "                    {\"role\": \"user\", \"content\": req[\"content\"]}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "        jsonl_lines.append(json.dumps(batch_request))\n",
        "\n",
        "    # Escribir archivo\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(jsonl_lines))\n",
        "\n",
        "    print(f'Archivo batch creado en: {output_path}')\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "sc7kOcC-xGct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "requests = [\n",
        "      {\"id\": \"req1\", \"content\": \"¬øQu√© es la inteligencia artificial?\"},\n",
        "      {\"id\": \"req2\", \"content\": \"Explica la diferencia entre machine learning y deep learning\"},\n",
        "      {\"id\": \"req3\", \"content\": \"¬øCu√°les son las aplicaciones del procesamiento de lenguaje natural?\"},\n",
        "      {\"id\": \"req4\", \"content\": \"Describe el funcionamiento b√°sico de ChatGPT\"},\n",
        "      {\"id\": \"req5\", \"content\": \"¬øQu√© es un modelo de embeddings y para qu√© sirve?\"}\n",
        "  ]\n",
        "\n",
        "  # Crear archivo de batch\n",
        "batch_file_path = create_batch_file(requests, '/content/openai_batch.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMBrRmnZxJrF",
        "outputId": "ee6b3739-0d37-47ee-cd61-f48a5bd6e4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando archivo de batch...\n",
            "Archivo batch creado en: /content/openai_batch.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "abQ_1KCQt1ZS",
        "outputId": "976b60c6-823b-430b-a4bd-d9cca777cd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/openai_batch.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Subir el archivo de entrada"
      ],
      "metadata": {
        "id": "rwgXslZksQhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Sube el archivo JSONL a OpenAI.\n",
        "\n",
        "    Args:\n",
        "        file_path: Ruta al archivo JSONL\n",
        "\n",
        "    Returns:\n",
        "        ID del archivo subido\n",
        "    \"\"\"\n",
        "    print('Subiendo archivo a OpenAI...')\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = client_openai.files.create(\n",
        "            file=f,\n",
        "            purpose=\"batch\",\n",
        "        )\n",
        "\n",
        "    file_id = response.id\n",
        "    print(f'Archivo subido con ID: {file_id}')\n",
        "    return file_id"
      ],
      "metadata": {
        "id": "P4phj-GCycCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = upload_file('/content/openai_batch.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX8D3TRXxkEv",
        "outputId": "960db9f4-f477-48bb-a3f9-2f10fe044a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subiendo archivo a OpenAI...\n",
            "Archivo subido con ID: file-8BD39uTxNdwEURFFgnGjGF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1CBSsrYOuNaf",
        "outputId": "442d35ef-ba79-4fd8-cd6e-4299bee60d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-8BD39uTxNdwEURFFgnGjGF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Crear nuestro lote (Batch)"
      ],
      "metadata": {
        "id": "V1szEHzasaU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(input_file_id: str):\n",
        "    \"\"\"\n",
        "    Crea un batch en OpenAI con el archivo subido.\n",
        "\n",
        "    Args:\n",
        "        input_file_id: ID del archivo subido a OpenAI\n",
        "\n",
        "    Returns:\n",
        "        Objeto batch creado\n",
        "    \"\"\"\n",
        "    print('Creando batch...')\n",
        "\n",
        "    batch = client_openai.batches.create(\n",
        "        input_file_id=input_file_id,\n",
        "        endpoint=\"/v1/chat/completions\",\n",
        "        completion_window=\"24h\",\n",
        "        metadata={\n",
        "            \"description\": \"Demo batch para video de YouTube\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f'Batch creado con ID: {batch.id}')\n",
        "    return batch"
      ],
      "metadata": {
        "id": "6BpRcfgJ2pBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_batch(file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ibXiJtP3EhC",
        "outputId": "4695a1cf-079f-4d33-fb78-0c901fc02e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando batch...\n",
            "Batch creado con ID: batch_682406de18708190bed4e66a925900b7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_682406de18708190bed4e66a925900b7', completion_window='24h', created_at=1747191518, endpoint='/v1/chat/completions', input_file_id='file-8BD39uTxNdwEURFFgnGjGF', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747277918, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Demo batch para video de YouTube'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Verificar el estado del lote\n"
      ],
      "metadata": {
        "id": "gkjZQgdTselv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPp0w6guvBLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = client_openai.batches.retrieve('batch_682406de18708190bed4e66a925900b7')\n",
        "batch"
      ],
      "metadata": {
        "id": "LXYzLp0_3sWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567e0fdc-c510-4b62-e61a-454a12a451fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_682406de18708190bed4e66a925900b7', completion_window='24h', created_at=1747191518, endpoint='/v1/chat/completions', input_file_id='file-8BD39uTxNdwEURFFgnGjGF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747191783, error_file_id=None, errors=None, expired_at=None, expires_at=1747277918, failed_at=None, finalizing_at=1747191782, in_progress_at=1747191584, metadata={'description': 'Demo batch para video de YouTube'}, output_file_id='file-NuLMpRvBYwqoTjdyjtmMrc', request_counts=BatchRequestCounts(completed=5, failed=0, total=5))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.output_file_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RF9FFTZagyg4",
        "outputId": "e67de6b7-7044-4d80-bbd5-98d70f0403a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-NuLMpRvBYwqoTjdyjtmMrc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Recuperar los resultados\n"
      ],
      "metadata": {
        "id": "Hv5M5zH2skv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_results(batch: Dict[str,Any]) -> None:\n",
        "    \"\"\"\n",
        "    Descarga y procesa los resultados del batch completado.\n",
        "\n",
        "    Args:\n",
        "        batch: Objeto batch completado\n",
        "    \"\"\"\n",
        "    if not batch.output_file_id:\n",
        "        print('No hay archivo de salida disponible.')\n",
        "        return\n",
        "\n",
        "    print(f'Descargando resultados desde archivo: {batch.output_file_id}')\n",
        "\n",
        "    try:\n",
        "        # Descargar archivo de resultados\n",
        "        output_response = client_openai.files.content(batch.output_file_id)\n",
        "\n",
        "        # Guardar resultados en archivo local\n",
        "        output_path = './batch_results.jsonl'\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(output_response.content)\n",
        "\n",
        "        print(f'Resultados guardados en: {output_path}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error al descargar resultados: {e}')"
      ],
      "metadata": {
        "id": "_YsmxaDb4ZXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_results(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQF0QyF04x_N",
        "outputId": "3eda2fd6-a3d9-4364-f275-2a3da272fe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando resultados desde archivo: file-NuLMpRvBYwqoTjdyjtmMrc\n",
            "Resultados guardados en: ./batch_results.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = '/content/batch_results.jsonl'\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "_9b5deVRg8el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFHzy3JNsq7y",
        "outputId": "3bb98804-ed34-473f-94e8-733d2c1a1698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'batch_req_682407e697348190adbca2b9e4058d2b',\n",
              "  'custom_id': 'req1',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'fc9979efb290f7389be4c638414b1f89',\n",
              "   'body': {'id': 'chatcmpl-BWwffJXKnQSBDCLBQHkGCVV4rwTXx',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¬°Arr, muchacho! La inteligencia artificial es como un loro bien entrenado, solo que en lugar de repetir palabras, este lorito met√°lico aprende y piensa por s√≠ mismo. Se trata de un conjunto de tecnolog√≠as que permiten a las m√°quinas a pensar y resolver problemas como si fuesen humanos, utilizando datos y algoritmos para hacerse m√°s listas y eficientes. As√≠ que ten cuidado, si te descuidas, ¬°podr√≠an hasta apoderarse de la nave! Arrr üè¥\\u200d‚ò†Ô∏è.',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 23,\n",
              "     'completion_tokens': 105,\n",
              "     'total_tokens': 128,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6a8408190967a3ade13a2e84f',\n",
              "  'custom_id': 'req2',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'e75092b0918919cb11bb4feffdd498a6',\n",
              "   'body': {'id': 'chatcmpl-BWwfhCk4p2rsYlzLZLk7gd4CmZdzr',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191729,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': \"¬°Argh, compa√±ero! Har√© lo mejor pa' explicarlo en nuestro lenguaje pirata. Imagina te que est√°s navegando en altamar. El machine learning vendr√≠a a ser como un mapa de esos que encontramos en una isla desierta, un mapa que nos dice hacia d√≥nde ir bas√°ndose en los tesoros que otros piratas ya han encontrado. Usa patrones previos pa' tomar decisiones y predecir ad√≥nde podr√≠an esconderse los tesoros.\\n\\nAhora, el deep learning, ¬°ah, es como tener un lorito sabio a tu lado que sabe de magia! Este lorito tiene una serie de capitanes peque√±itos en su cabeza, a los que llamamos capas, y cada uno toma decisiones basadas en lo que le dice el lorito anterior. Puede resolver acertijos m√°s complejos y seguir pistas que son m√°s dif√≠ciles pa' la mayor√≠a de nosotros. A diferencia del mapa, que necesita que ya alguien haya pasado por ah√≠ antes, el lorito puede aprender por s√≠ mismo si ve suficientes pistas.\\n\\nEn resumen, el machine learning es como un mapa b√°sico que necesita de un poco de gu√≠a humana, mientras que el deep learning es como un lorito m√°gico que aprende solo y puede ayudarte a encontrar tesoros escondidos m√°s complicados. ¬°Argh! ¬°Espero que esta explicaci√≥n sea tan clara como el agua del Caribe!\",\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 27,\n",
              "     'completion_tokens': 276,\n",
              "     'total_tokens': 303,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6ba508190b2dc348e7343e876',\n",
              "  'custom_id': 'req3',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': '22169deca0ecd9ae47089656b332ed9a',\n",
              "   'body': {'id': 'chatcmpl-BWwffqvJ7cPwltBiaigDDMqMS11UJ',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¬°Ahoy, camarada del ciberespacio! El procesamiento de lenguaje natural, o PLN como dicen los sabios de la tecnolog√≠a, tiene m√°s usos que un cofre de tesoros en un gale√≥n pirata. Aqu√≠ te cuento algunos, ¬°argh!\\n\\n1. **Chatbots y asistentes virtuales** - Son como grumetes que entienden y responden a las preguntas del p√∫blico, as√≠ como los loros repiten las palabras de un buen pirata.\\n\\n2. **Traducci√≥n autom√°tica** - Igual que un cart√≥grafo con mapas, esto convierte las palabras de un idioma a otro sin necesidad de atravesar oc√©anos.\\n\\n3. **An√°lisis de sentimientos** - Para conocer si el viento sopla a favor o en contra en cuanto a las emociones de los clientes en las redes sociales o rese√±as.\\n\\n4. **Reconocimiento de voz** - Como si oyera el eco de una voz de sirena, esta t√©cnica convierte palabras habladas en texto.\\n\\n5. **Resumen de textos** - Estilo corsario, toma largas parrafadas y las resume, dej√°ndolo tan claro como las aguas del Caribe.\\n\\n6. **Filtrado de spam** - Como un cazatesoros que detecta mensajes indeseados, manteniendo las bandejas libres de basura.\\n\\n7. **Extracci√≥n de informaci√≥n** - Como un bucanero experto en encontrar gemas escondidas entre monta√±as de texto.\\n\\n8. **Correcci√≥n de gram√°tica** - Que revisen tus parrafadas m√°s r√°pido que un ca√±onazo en pleno abordaje.\\n\\nEstos son solo algunos ejemplos, ¬°pero las posibilidades son m√°s vastas que el oc√©ano mismo! ¬°Argh, y que los vientos de la innovaci√≥n nos gu√≠en! üè¥\\u200d‚ò†Ô∏è',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 28,\n",
              "     'completion_tokens': 366,\n",
              "     'total_tokens': 394,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_90122d973c'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6ca608190bec4cf7c2784ab6c',\n",
              "  'custom_id': 'req4',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': 'c5babe5bf7feeaa72cda3467cf12c5c1',\n",
              "   'body': {'id': 'chatcmpl-BWwffqsRraBtFGBaKBwXSSccf1KWW',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191727,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¬°Argh, mi amigo! Perm√≠teme contarte el relato sobre c√≥mo funciona este tabernero virtual, ChatGPT. Este artilugio ingenioso es un modelo de lenguaje que ha sido entrenado con montones de datos, cual cofre del tesoro repleto de oro y mapas del conocimiento humano.\\n\\nPara empezar, cuando hac√©is una pregunta, este bucanero de la inteligencia artificial la recibe, procesando cada palabra como si de un mapa del tesoro se tratara. Luego, escudri√±a sus archivos y patrones, buscando las gemas de la respuesta m√°s adecuada para ti. Lo hace prediciendo las palabras que vendr√°n a continuaci√≥n, bas√°ndose en su vasto conocimiento, de forma similar a c√≥mo un viejo lobo de mar orienta su barco en la b√∫squeda de riquezas.\\n\\nY no creas que este corsario tiene cada respuesta grabada en piedra como un pergamino de un hechicero; m√°s bien, genera las respuestas en el acto, siempre atento a los mares de informaci√≥n y ajustando el tim√≥n para ofrecer una respuesta propia y contextualizada, como si su br√∫jula girara buscando el norte.\\n\\nAs√≠ que, amigo, cada vez que conversas con este bot, est√°s navegando en un oc√©ano de informaci√≥n, ¬°y su prop√≥sito es guiarte con sus palabras hasta la isla del entendimiento! ¬°Arrr!',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 24,\n",
              "     'completion_tokens': 285,\n",
              "     'total_tokens': 309,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_f5bdcc3276'}},\n",
              "  'error': None},\n",
              " {'id': 'batch_req_682407e6dd78819096d386fe46aee3e7',\n",
              "  'custom_id': 'req5',\n",
              "  'response': {'status_code': 200,\n",
              "   'request_id': '1190f2d4fe04f13db0707d39c935ca5f',\n",
              "   'body': {'id': 'chatcmpl-BWwfghHEoiU12RdXaz5CDMheG9jJg',\n",
              "    'object': 'chat.completion',\n",
              "    'created': 1747191728,\n",
              "    'model': 'gpt-4o-2024-08-06',\n",
              "    'choices': [{'index': 0,\n",
              "      'message': {'role': 'assistant',\n",
              "       'content': '¬°Argh, grumete! Un modelo de embeddings es como el mapa del tesoro para las palabras en el mundo de la inteligencia artificial. En lugar de trabajar con las palabras como simples palabras, esos modelos las convierten en vectores, que vienen siendo como coordenadas en un vasto mar de posibilidades. Estos vectores capturan el significado y la relaci√≥n entre las palabras, navegando por los mares de datos con precisi√≥n.\\n\\n¬øY para qu√© sirve, preguntas? ¬°Para encontrar tesoros ocultos en los datos, claro est√°! Se utilizan en cosas como traducci√≥n de lenguajes, b√∫squeda de informaci√≥n, y procesamiento de lenguaje natural. Con estos modelos, podemos surcar las aguas del conocimiento, encontrando conexiones entre palabras que ni las sirenas habr√≠an imaginado. ¬°As√≠ que a izar las velas y descubrir los secretos que esos embeddings tienen escondidos! ¬°Argh! ‚ò†Ô∏èüè¥\\u200d‚ò†Ô∏è',\n",
              "       'refusal': None,\n",
              "       'annotations': []},\n",
              "      'logprobs': None,\n",
              "      'finish_reason': 'stop'}],\n",
              "    'usage': {'prompt_tokens': 28,\n",
              "     'completion_tokens': 193,\n",
              "     'total_tokens': 221,\n",
              "     'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "     'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "      'audio_tokens': 0,\n",
              "      'accepted_prediction_tokens': 0,\n",
              "      'rejected_prediction_tokens': 0}},\n",
              "    'service_tier': 'default',\n",
              "    'system_fingerprint': 'fp_d8864f8b6b'}},\n",
              "  'error': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "responde_df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "jBEerlZThysq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responde_df['response'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5d6Kz5vh0mc",
        "outputId": "5bcabe17-bd96-448b-fe65-58d5de6b8045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status_code': 200,\n",
              " 'request_id': 'fc9979efb290f7389be4c638414b1f89',\n",
              " 'body': {'id': 'chatcmpl-BWwffJXKnQSBDCLBQHkGCVV4rwTXx',\n",
              "  'object': 'chat.completion',\n",
              "  'created': 1747191727,\n",
              "  'model': 'gpt-4o-2024-08-06',\n",
              "  'choices': [{'index': 0,\n",
              "    'message': {'role': 'assistant',\n",
              "     'content': '¬°Arr, muchacho! La inteligencia artificial es como un loro bien entrenado, solo que en lugar de repetir palabras, este lorito met√°lico aprende y piensa por s√≠ mismo. Se trata de un conjunto de tecnolog√≠as que permiten a las m√°quinas a pensar y resolver problemas como si fuesen humanos, utilizando datos y algoritmos para hacerse m√°s listas y eficientes. As√≠ que ten cuidado, si te descuidas, ¬°podr√≠an hasta apoderarse de la nave! Arrr üè¥\\u200d‚ò†Ô∏è.',\n",
              "     'refusal': None,\n",
              "     'annotations': []},\n",
              "    'logprobs': None,\n",
              "    'finish_reason': 'stop'}],\n",
              "  'usage': {'prompt_tokens': 23,\n",
              "   'completion_tokens': 105,\n",
              "   'total_tokens': 128,\n",
              "   'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
              "   'completion_tokens_details': {'reasoning_tokens': 0,\n",
              "    'audio_tokens': 0,\n",
              "    'accepted_prediction_tokens': 0,\n",
              "    'rejected_prediction_tokens': 0}},\n",
              "  'service_tier': 'default',\n",
              "  'system_fingerprint': 'fp_d8864f8b6b'}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}